from bs4 import BeautifulSoup
import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample HTML content
html_content = """
<html>
<body>
<p>This is the first paragraph. It contains some words.</p>
<p>This is the second paragraph. It has some different words.</p>
<p>This is the third paragraph. It also contains some words.</p>
</body>
</html>
"""

# Parse the HTML content
soup = BeautifulSoup(html_content, "html.parser")

# Extract text from paragraphs
paragraphs = [p.get_text() for p in soup.find_all("p")]

# Tokenize and preprocess text
doc1 = nlp(paragraphs[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Calculate similarity and highlight similar words
for paragraph in paragraphs[1:]:
    doc2 = nlp(paragraph)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Highlight similar words
    highlighted_paragraph = ""
    for token in doc2:
        if token.text.lower() in tokens1:
            highlighted_paragraph += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_paragraph += f"{token.text} "
    
    print(f"Similarity: {similarity}")
    print(f"Highlighted Paragraph: {highlighted_paragraph}")


----------
# Assuming paragraphs are within <div> tags with class="paragraph"
divs = soup.find_all("div", class_="paragraph")
paragraphs = [div.get_text() for div in divs]

# Rest of the code remains the same




--+++++------



import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample HTML content
html_content = """
<html>
<body>
<p>This is the first line. It contains some words.</p>
<p>This is the second line. It has some different words.</p>
<p>This is the third line. It also contains some words.</p>
</body>
</html>
"""

# Extract lines from the HTML content
lines = [line.strip() for line in html_content.split("\n") if line.strip()]

# Tokenize and preprocess text
doc1 = nlp(lines[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Calculate similarity and highlight similar words
for line in lines[1:]:
    doc2 = nlp(line)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Highlight similar words
    highlighted_line = ""
    for token in doc2:
        if token.text.lower() in tokens1:
            highlighted_line += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_line += f"{token.text} "
    
    print(f"Similarity: {similarity}")
    print(f"Highlighted Line: {highlighted_line}")



----------
import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Read the HTML file
with open("example.htm", "r") as file:
    html_content = file.read()

# Extract lines from the HTML content
lines = [line.strip() for line in html_content.split("\n") if line.strip()]

# Tokenize and preprocess text
doc1 = nlp(lines[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Set to store similar words
similar_words = set()

# Calculate similarity and store similar words
for line in lines[1:]:
    doc2 = nlp(line)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Add similar words to the set
    similar_words.update(tokens2.intersection(tokens1))

# Parse through lines and highlight similar words
for line in lines:
    doc = nlp(line)
    highlighted_line = ""
    for token in doc:
        if token.text.lower() in similar_words:
            highlighted_line += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_line += f"{token.text} "
    
    print(f"Highlighted Line: {highlighted_line}")



