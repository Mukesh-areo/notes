from bs4 import BeautifulSoup
import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample HTML content
html_content = """
<html>
<body>
<p>This is the first paragraph. It contains some words.</p>
<p>This is the second paragraph. It has some different words.</p>
<p>This is the third paragraph. It also contains some words.</p>
</body>
</html>
"""

# Parse the HTML content
soup = BeautifulSoup(html_content, "html.parser")

# Extract text from paragraphs
paragraphs = [p.get_text() for p in soup.find_all("p")]

# Tokenize and preprocess text
doc1 = nlp(paragraphs[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Calculate similarity and highlight similar words
for paragraph in paragraphs[1:]:
    doc2 = nlp(paragraph)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Highlight similar words
    highlighted_paragraph = ""
    for token in doc2:
        if token.text.lower() in tokens1:
            highlighted_paragraph += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_paragraph += f"{token.text} "
    
    print(f"Similarity: {similarity}")
    print(f"Highlighted Paragraph: {highlighted_paragraph}")


----------
# Assuming paragraphs are within <div> tags with class="paragraph"
divs = soup.find_all("div", class_="paragraph")
paragraphs = [div.get_text() for div in divs]

# Rest of the code remains the same




--+++++------



import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample HTML content
html_content = """
<html>
<body>
<p>This is the first line. It contains some words.</p>
<p>This is the second line. It has some different words.</p>
<p>This is the third line. It also contains some words.</p>
</body>
</html>
"""

# Extract lines from the HTML content
lines = [line.strip() for line in html_content.split("\n") if line.strip()]

# Tokenize and preprocess text
doc1 = nlp(lines[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Calculate similarity and highlight similar words
for line in lines[1:]:
    doc2 = nlp(line)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Highlight similar words
    highlighted_line = ""
    for token in doc2:
        if token.text.lower() in tokens1:
            highlighted_line += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_line += f"{token.text} "
    
    print(f"Similarity: {similarity}")
    print(f"Highlighted Line: {highlighted_line}")



----------
import spacy

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Read the HTML file
with open("example.htm", "r") as file:
    html_content = file.read()

# Extract lines from the HTML content
lines = [line.strip() for line in html_content.split("\n") if line.strip()]

# Tokenize and preprocess text
doc1 = nlp(lines[0])
tokens1 = set(token.text.lower() for token in doc1 if token.is_alpha)

# Set to store similar words
similar_words = set()

# Calculate similarity and store similar words
for line in lines[1:]:
    doc2 = nlp(line)
    tokens2 = set(token.text.lower() for token in doc2 if token.is_alpha)
    
    # Calculate similarity
    similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))
    
    # Add similar words to the set
    similar_words.update(tokens2.intersection(tokens1))

# Parse through lines and highlight similar words
for line in lines:
    doc = nlp(line)
    highlighted_line = ""
    for token in doc:
        if token.text.lower() in similar_words:
            highlighted_line += f"<span style='background-color: yellow;'>{token.text}</span> "
        else:
            highlighted_line += f"{token.text} "
    
    print(f"Highlighted Line: {highlighted_line}")







_------------------


from bs4 import BeautifulSoup

def highlight_repeated_content(html_file):
    with open(html_file, 'r', encoding='utf-8') as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, 'html.parser')
    divs = soup.find_all('div')

    for i, div in enumerate(divs):
        for other_div in divs[i+1:]:
            if div.text == other_div.text:
                div['style'] = 'background-color: yellow;'

    with open('highlighted.html', 'w', encoding='utf-8') as file:
        file.write(str(soup))

highlight_repeated_content('your_html_file.html')



______&&&&&&&&+-------
from bs4 import BeautifulSoup
import hashlib
import random
import string

def get_random_color():
    return "#{:06x}".format(random.randint(0, 0xFFFFFF))

def highlight_repeated_content(html_file):
    with open(html_file, 'r', encoding='utf-8') as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, 'html.parser')
    divs = soup.find_all('div')
    highlighted_content = {}

    for i, div in enumerate(divs):
        content_hash = hashlib.sha256(div.text.encode()).hexdigest()
        if content_hash in highlighted_content:
            div['style'] = 'background-color: {}'.format(highlighted_content[content_hash])
        else:
            color = get_random_color()
            highlighted_content[content_hash] = color
            div['style'] = 'background-color: {}'.format(color)

    with open('highlighted.html', 'w', encoding='utf-8') as file:
        file.write(str(soup))

highlight_repeated_content('your_html_file.html')




-----





